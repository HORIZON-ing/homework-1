2023-02-06 17:13:54,331 - mmcls - INFO - Environment info:
------------------------------------------------------------
sys.platform: linux
Python: 3.8.16 (default, Jan 17 2023, 23:13:24) [GCC 11.2.0]
CUDA available: False
GCC: gcc (Ubuntu 7.5.0-3ubuntu1~18.04) 7.5.0
PyTorch: 1.12.1
PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201402
  - Intel(R) oneAPI Math Kernel Library Version 2021.4-Product Build 20210904 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.6.0 (Git Hash 52b5f107dd9cf10910aaa19cb47f3abf9b349815)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX2
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.3, CUDNN_VERSION=8.3.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_KINETO -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -DEDGE_PROFILER_USE_KINETO -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=1.12.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=OFF, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 

TorchVision: 0.13.1
OpenCV: 4.7.0
MMCV: 1.7.1
MMCV Compiler: GCC 9.3
MMCV CUDA Compiler: 11.3
MMClassification: 0.25.0+
------------------------------------------------------------

2023-02-06 17:13:54,331 - mmcls - INFO - Distributed training: False
2023-02-06 17:13:54,451 - mmcls - INFO - Config:
model = dict(
    type='ImageClassifier',
    backbone=dict(
        type='SwinTransformerV2',
        arch='tiny',
        img_size=256,
        drop_path_rate=0.2,
        pad_small_map=True),
    neck=dict(type='GlobalAveragePooling'),
    head=dict(
        type='LinearClsHead',
        num_classes=5,
        in_channels=768,
        init_cfg=None,
        loss=dict(
            type='LabelSmoothLoss', label_smooth_val=0.1, mode='original'),
        cal_acc=False,
        topk=(1, )),
    init_cfg=[
        dict(type='TruncNormal', layer='Linear', std=0.02, bias=0.0),
        dict(type='Constant', layer='LayerNorm', val=1.0, bias=0.0)
    ],
    train_cfg=dict(augments=[
        dict(type='BatchMixup', alpha=0.8, num_classes=5, prob=0.5),
        dict(type='BatchCutMix', alpha=1.0, num_classes=5, prob=0.5)
    ]))
dataset_type = 'ImageNet'
img_norm_cfg = dict(
    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='RandomResizedCrop', size=224),
    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='ToTensor', keys=['gt_label']),
    dict(type='Collect', keys=['img', 'gt_label'])
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', size=(256, -1)),
    dict(type='CenterCrop', crop_size=224),
    dict(
        type='Normalize',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        to_rgb=True),
    dict(type='ImageToTensor', keys=['img']),
    dict(type='Collect', keys=['img'])
]
data = dict(
    samples_per_gpu=8,
    workers_per_gpu=2,
    train=dict(
        type='ImageNet',
        data_prefix='data/flower_dataset/train',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='RandomResizedCrop', size=224),
            dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='ToTensor', keys=['gt_label']),
            dict(type='Collect', keys=['img', 'gt_label'])
        ],
        ann_file='data/flower_dataset/train.txt',
        classes='data/flower_dataset/classes.txt'),
    val=dict(
        type='ImageNet',
        data_prefix='data/flower_dataset/val',
        ann_file='data/flower_dataset/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', size=(256, -1)),
            dict(type='CenterCrop', crop_size=224),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ],
        classes='data/flower_dataset/classes.txt'),
    test=dict(
        type='ImageNet',
        data_prefix='data/imagenet/val',
        ann_file='data/imagenet/meta/val.txt',
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', size=(256, -1)),
            dict(type='CenterCrop', crop_size=224),
            dict(
                type='Normalize',
                mean=[123.675, 116.28, 103.53],
                std=[58.395, 57.12, 57.375],
                to_rgb=True),
            dict(type='ImageToTensor', keys=['img']),
            dict(type='Collect', keys=['img'])
        ]))
evaluation = dict(interval=1, metric='accuracy')
checkpoint_config = dict(interval=1)
log_config = dict(interval=100, hooks=[dict(type='TextLoggerHook')])
dist_params = dict(backend='nccl')
log_level = 'INFO'
load_from = '/home/gjq/mmclassification/checkpoints/swinv2-tiny-w16_3rdparty_in1k-256px_20220803-9651cdd7.pth'
resume_from = None
workflow = [('train', 1)]
optimizer = dict(type='SGD', lr=0.001, momentum=0.9, weight_decay=0.0001)
optimizer_config = dict(grad_clip=None)
lr_config = dict(policy='step', step=[1])
runner = dict(type='EpochBasedRunner', max_epochs=100)
work_dir = 'work_dirs/flower_swimv2_tiny'
gpu_ids = [0]

2023-02-06 17:13:54,451 - mmcls - INFO - Set random seed to 702594236, deterministic: False
2023-02-06 17:13:54,823 - mmcls - INFO - load checkpoint from local path: /home/gjq/mmclassification/checkpoints/swinv2-tiny-w16_3rdparty_in1k-256px_20220803-9651cdd7.pth
2023-02-06 17:13:54,880 - mmcls - WARNING - The model and loaded state dict do not match exactly

size mismatch for head.fc.weight: copying a param with shape torch.Size([1000, 768]) from checkpoint, the shape in current model is torch.Size([5, 768]).
size mismatch for head.fc.bias: copying a param with shape torch.Size([1000]) from checkpoint, the shape in current model is torch.Size([5]).
missing keys in source state_dict: backbone.stages.0.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.0.attn.w_msa.relative_position_index, backbone.stages.0.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.0.blocks.1.attn.w_msa.relative_position_index, backbone.stages.1.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.0.attn.w_msa.relative_position_index, backbone.stages.1.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.1.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.0.attn.w_msa.relative_position_index, backbone.stages.2.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.1.attn.w_msa.relative_position_index, backbone.stages.2.blocks.2.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.2.attn.w_msa.relative_position_index, backbone.stages.2.blocks.3.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.3.attn.w_msa.relative_position_index, backbone.stages.2.blocks.4.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.4.attn.w_msa.relative_position_index, backbone.stages.2.blocks.5.attn.w_msa.relative_coords_table, backbone.stages.2.blocks.5.attn.w_msa.relative_position_index, backbone.stages.3.blocks.0.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.0.attn.w_msa.relative_position_index, backbone.stages.3.blocks.1.attn.w_msa.relative_coords_table, backbone.stages.3.blocks.1.attn.w_msa.relative_position_index

2023-02-06 17:13:54,882 - mmcls - INFO - Start running, host: gjq@gjq-ROG16, work_dir: /home/gjq/mmclassification/work_dirs/flower_swimv2_tiny
2023-02-06 17:13:54,882 - mmcls - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) StepLrUpdaterHook                  
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_train_iter:
(VERY_HIGH   ) StepLrUpdaterHook                  
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
 -------------------- 
after_train_iter:
(ABOVE_NORMAL) OptimizerHook                      
(NORMAL      ) CheckpointHook                     
(LOW         ) IterTimerHook                      
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) CheckpointHook                     
(LOW         ) EvalHook                           
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_epoch:
(LOW         ) IterTimerHook                      
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
before_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_iter:
(LOW         ) IterTimerHook                      
 -------------------- 
after_val_epoch:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
after_run:
(VERY_LOW    ) TextLoggerHook                     
 -------------------- 
2023-02-06 17:13:54,882 - mmcls - INFO - workflow: [('train', 1)], max: 100 epochs
2023-02-06 17:13:54,882 - mmcls - INFO - Checkpoints will be saved to /home/gjq/mmclassification/work_dirs/flower_swimv2_tiny by HardDiskBackend.
2023-02-06 17:16:09,738 - mmcls - INFO - Epoch [1][100/285]	lr: 1.000e-03, eta: 10:38:18, time: 1.349, data_time: 0.022, loss: 1.4811
2023-02-06 17:18:31,838 - mmcls - INFO - Epoch [1][200/285]	lr: 1.000e-03, eta: 10:53:08, time: 1.421, data_time: 0.002, loss: 1.2350
2023-02-06 17:20:31,155 - mmcls - INFO - Saving checkpoint at 1 epochs
2023-02-06 17:21:08,527 - mmcls - INFO - Epoch(val) [1][72]	accuracy_top-1: 83.3916, accuracy_top-5: 100.0000
2023-02-06 17:23:33,282 - mmcls - INFO - Epoch [2][100/285]	lr: 1.000e-04, eta: 8:33:15, time: 1.448, data_time: 0.022, loss: 1.0951
2023-02-06 17:25:55,272 - mmcls - INFO - Epoch [2][200/285]	lr: 1.000e-04, eta: 9:02:40, time: 1.420, data_time: 0.002, loss: 1.0531
2023-02-06 17:27:55,631 - mmcls - INFO - Saving checkpoint at 2 epochs
2023-02-06 17:28:33,060 - mmcls - INFO - Epoch(val) [2][72]	accuracy_top-1: 92.1329, accuracy_top-5: 100.0000
2023-02-06 17:30:56,529 - mmcls - INFO - Epoch [3][100/285]	lr: 1.000e-04, eta: 8:09:33, time: 1.435, data_time: 0.022, loss: 1.0162
2023-02-06 17:33:18,681 - mmcls - INFO - Epoch [3][200/285]	lr: 1.000e-04, eta: 8:29:46, time: 1.422, data_time: 0.002, loss: 1.0098
2023-02-06 17:35:19,205 - mmcls - INFO - Saving checkpoint at 3 epochs
2023-02-06 17:35:56,656 - mmcls - INFO - Epoch(val) [3][72]	accuracy_top-1: 93.3566, accuracy_top-5: 100.0000
2023-02-06 17:38:20,255 - mmcls - INFO - Epoch [4][100/285]	lr: 1.000e-04, eta: 7:57:18, time: 1.436, data_time: 0.022, loss: 1.0200
2023-02-06 17:40:42,729 - mmcls - INFO - Epoch [4][200/285]	lr: 1.000e-04, eta: 8:12:16, time: 1.425, data_time: 0.002, loss: 1.0146
2023-02-06 17:42:42,907 - mmcls - INFO - Saving checkpoint at 4 epochs
2023-02-06 17:43:20,399 - mmcls - INFO - Epoch(val) [4][72]	accuracy_top-1: 94.0559, accuracy_top-5: 100.0000
2023-02-06 17:45:43,941 - mmcls - INFO - Epoch [5][100/285]	lr: 1.000e-04, eta: 7:48:35, time: 1.435, data_time: 0.022, loss: 1.0082
2023-02-06 17:48:06,770 - mmcls - INFO - Epoch [5][200/285]	lr: 1.000e-04, eta: 8:00:16, time: 1.428, data_time: 0.002, loss: 0.9812
2023-02-06 17:50:07,450 - mmcls - INFO - Saving checkpoint at 5 epochs
2023-02-06 17:50:44,904 - mmcls - INFO - Epoch(val) [5][72]	accuracy_top-1: 94.7552, accuracy_top-5: 100.0000
2023-02-06 17:53:08,749 - mmcls - INFO - Epoch [6][100/285]	lr: 1.000e-04, eta: 7:41:33, time: 1.438, data_time: 0.022, loss: 0.9855
2023-02-06 17:55:31,475 - mmcls - INFO - Epoch [6][200/285]	lr: 1.000e-04, eta: 7:50:52, time: 1.427, data_time: 0.002, loss: 1.0041
2023-02-06 17:57:32,155 - mmcls - INFO - Saving checkpoint at 6 epochs
2023-02-06 17:58:09,852 - mmcls - INFO - Epoch(val) [6][72]	accuracy_top-1: 93.7063, accuracy_top-5: 100.0000
2023-02-06 18:00:34,329 - mmcls - INFO - Epoch [7][100/285]	lr: 1.000e-04, eta: 7:35:21, time: 1.445, data_time: 0.022, loss: 0.9832
2023-02-06 18:02:57,376 - mmcls - INFO - Epoch [7][200/285]	lr: 1.000e-04, eta: 7:43:05, time: 1.430, data_time: 0.002, loss: 0.9602
2023-02-06 18:04:58,473 - mmcls - INFO - Saving checkpoint at 7 epochs
2023-02-06 18:05:36,047 - mmcls - INFO - Epoch(val) [7][72]	accuracy_top-1: 94.9301, accuracy_top-5: 100.0000
2023-02-06 18:08:00,451 - mmcls - INFO - Epoch [8][100/285]	lr: 1.000e-04, eta: 7:29:35, time: 1.444, data_time: 0.022, loss: 0.9449
2023-02-06 18:10:22,757 - mmcls - INFO - Epoch [8][200/285]	lr: 1.000e-04, eta: 7:35:54, time: 1.423, data_time: 0.002, loss: 0.9745
2023-02-06 18:12:23,571 - mmcls - INFO - Saving checkpoint at 8 epochs
2023-02-06 18:13:01,267 - mmcls - INFO - Epoch(val) [8][72]	accuracy_top-1: 95.6294, accuracy_top-5: 100.0000
2023-02-06 18:15:25,201 - mmcls - INFO - Epoch [9][100/285]	lr: 1.000e-04, eta: 7:23:50, time: 1.439, data_time: 0.022, loss: 0.9538
2023-02-06 18:17:48,376 - mmcls - INFO - Epoch [9][200/285]	lr: 1.000e-04, eta: 7:29:20, time: 1.432, data_time: 0.002, loss: 0.9757
2023-02-06 18:19:48,815 - mmcls - INFO - Saving checkpoint at 9 epochs
2023-02-06 18:20:26,623 - mmcls - INFO - Epoch(val) [9][72]	accuracy_top-1: 94.7552, accuracy_top-5: 100.0000
2023-02-06 18:22:49,908 - mmcls - INFO - Epoch [10][100/285]	lr: 1.000e-04, eta: 7:18:19, time: 1.433, data_time: 0.022, loss: 0.9514
2023-02-06 18:25:12,575 - mmcls - INFO - Epoch [10][200/285]	lr: 1.000e-04, eta: 7:22:58, time: 1.427, data_time: 0.002, loss: 0.9819
2023-02-06 18:27:12,612 - mmcls - INFO - Saving checkpoint at 10 epochs
2023-02-06 18:27:50,097 - mmcls - INFO - Epoch(val) [10][72]	accuracy_top-1: 96.1538, accuracy_top-5: 100.0000
2023-02-06 18:30:14,443 - mmcls - INFO - Epoch [11][100/285]	lr: 1.000e-04, eta: 7:13:02, time: 1.443, data_time: 0.022, loss: 0.9534
2023-02-06 18:32:37,291 - mmcls - INFO - Epoch [11][200/285]	lr: 1.000e-04, eta: 7:17:04, time: 1.428, data_time: 0.002, loss: 0.9523
2023-02-06 18:34:37,693 - mmcls - INFO - Saving checkpoint at 11 epochs
2023-02-06 18:35:15,469 - mmcls - INFO - Epoch(val) [11][72]	accuracy_top-1: 94.9301, accuracy_top-5: 100.0000
2023-02-06 18:37:39,258 - mmcls - INFO - Epoch [12][100/285]	lr: 1.000e-04, eta: 7:07:47, time: 1.438, data_time: 0.022, loss: 0.9160
2023-02-06 18:40:01,649 - mmcls - INFO - Epoch [12][200/285]	lr: 1.000e-04, eta: 7:11:14, time: 1.424, data_time: 0.002, loss: 0.9708
2023-02-06 18:42:01,678 - mmcls - INFO - Saving checkpoint at 12 epochs
2023-02-06 18:42:39,267 - mmcls - INFO - Epoch(val) [12][72]	accuracy_top-1: 96.5035, accuracy_top-5: 100.0000
2023-02-06 18:45:03,076 - mmcls - INFO - Epoch [13][100/285]	lr: 1.000e-04, eta: 7:02:34, time: 1.438, data_time: 0.022, loss: 0.9409
2023-02-06 18:47:25,952 - mmcls - INFO - Epoch [13][200/285]	lr: 1.000e-04, eta: 7:05:37, time: 1.429, data_time: 0.002, loss: 0.9350
2023-02-06 18:49:26,820 - mmcls - INFO - Saving checkpoint at 13 epochs
2023-02-06 18:50:04,531 - mmcls - INFO - Epoch(val) [13][72]	accuracy_top-1: 96.5035, accuracy_top-5: 100.0000
2023-02-06 18:52:28,454 - mmcls - INFO - Epoch [14][100/285]	lr: 1.000e-04, eta: 6:57:29, time: 1.439, data_time: 0.022, loss: 0.9498
2023-02-06 18:54:50,824 - mmcls - INFO - Epoch [14][200/285]	lr: 1.000e-04, eta: 7:00:05, time: 1.424, data_time: 0.002, loss: 0.9404
2023-02-06 18:56:51,208 - mmcls - INFO - Saving checkpoint at 14 epochs
2023-02-06 18:57:28,940 - mmcls - INFO - Epoch(val) [14][72]	accuracy_top-1: 96.3287, accuracy_top-5: 100.0000
2023-02-06 18:59:52,953 - mmcls - INFO - Epoch [15][100/285]	lr: 1.000e-04, eta: 6:52:23, time: 1.440, data_time: 0.022, loss: 0.9429
2023-02-06 19:02:15,294 - mmcls - INFO - Epoch [15][200/285]	lr: 1.000e-04, eta: 6:54:40, time: 1.423, data_time: 0.002, loss: 0.9375
2023-02-06 19:04:15,404 - mmcls - INFO - Saving checkpoint at 15 epochs
2023-02-06 19:04:52,856 - mmcls - INFO - Epoch(val) [15][72]	accuracy_top-1: 96.5035, accuracy_top-5: 100.0000
2023-02-06 19:07:16,898 - mmcls - INFO - Epoch [16][100/285]	lr: 1.000e-04, eta: 6:47:21, time: 1.440, data_time: 0.022, loss: 0.9252
2023-02-06 19:09:39,669 - mmcls - INFO - Epoch [16][200/285]	lr: 1.000e-04, eta: 6:49:22, time: 1.428, data_time: 0.002, loss: 0.9315
2023-02-06 19:11:39,766 - mmcls - INFO - Saving checkpoint at 16 epochs
2023-02-06 19:12:17,302 - mmcls - INFO - Epoch(val) [16][72]	accuracy_top-1: 96.5035, accuracy_top-5: 100.0000
2023-02-06 19:14:41,049 - mmcls - INFO - Epoch [17][100/285]	lr: 1.000e-04, eta: 6:42:21, time: 1.437, data_time: 0.022, loss: 0.9229
2023-02-06 19:17:03,916 - mmcls - INFO - Epoch [17][200/285]	lr: 1.000e-04, eta: 6:44:07, time: 1.429, data_time: 0.002, loss: 0.9502
2023-02-06 19:19:04,180 - mmcls - INFO - Saving checkpoint at 17 epochs
2023-02-06 19:19:41,746 - mmcls - INFO - Epoch(val) [17][72]	accuracy_top-1: 96.5035, accuracy_top-5: 100.0000
2023-02-06 19:22:05,796 - mmcls - INFO - Epoch [18][100/285]	lr: 1.000e-04, eta: 6:37:24, time: 1.440, data_time: 0.022, loss: 0.9364
2023-02-06 19:24:28,133 - mmcls - INFO - Epoch [18][200/285]	lr: 1.000e-04, eta: 6:38:54, time: 1.423, data_time: 0.002, loss: 0.9502
2023-02-06 19:26:28,268 - mmcls - INFO - Saving checkpoint at 18 epochs
2023-02-06 19:27:05,786 - mmcls - INFO - Epoch(val) [18][72]	accuracy_top-1: 96.1538, accuracy_top-5: 100.0000
2023-02-06 19:29:29,882 - mmcls - INFO - Epoch [19][100/285]	lr: 1.000e-04, eta: 6:32:26, time: 1.441, data_time: 0.022, loss: 0.9323
2023-02-06 19:31:52,164 - mmcls - INFO - Epoch [19][200/285]	lr: 1.000e-04, eta: 6:33:44, time: 1.423, data_time: 0.002, loss: 0.9174
2023-02-06 19:33:52,480 - mmcls - INFO - Saving checkpoint at 19 epochs
2023-02-06 19:34:29,889 - mmcls - INFO - Epoch(val) [19][72]	accuracy_top-1: 95.2797, accuracy_top-5: 100.0000
2023-02-06 19:36:53,675 - mmcls - INFO - Epoch [20][100/285]	lr: 1.000e-04, eta: 6:27:28, time: 1.438, data_time: 0.022, loss: 0.9010
2023-02-06 19:39:15,939 - mmcls - INFO - Epoch [20][200/285]	lr: 1.000e-04, eta: 6:28:35, time: 1.423, data_time: 0.002, loss: 0.9217
2023-02-06 19:41:16,561 - mmcls - INFO - Saving checkpoint at 20 epochs
2023-02-06 19:41:54,195 - mmcls - INFO - Epoch(val) [20][72]	accuracy_top-1: 96.8531, accuracy_top-5: 100.0000
2023-02-06 19:44:18,100 - mmcls - INFO - Epoch [21][100/285]	lr: 1.000e-04, eta: 6:22:32, time: 1.439, data_time: 0.022, loss: 0.9005
2023-02-06 19:46:40,471 - mmcls - INFO - Epoch [21][200/285]	lr: 1.000e-04, eta: 6:23:29, time: 1.424, data_time: 0.002, loss: 0.9245
2023-02-06 19:48:41,088 - mmcls - INFO - Saving checkpoint at 21 epochs
2023-02-06 19:49:18,725 - mmcls - INFO - Epoch(val) [21][72]	accuracy_top-1: 94.5804, accuracy_top-5: 100.0000
2023-02-06 19:51:42,363 - mmcls - INFO - Epoch [22][100/285]	lr: 1.000e-04, eta: 6:17:35, time: 1.436, data_time: 0.022, loss: 0.9321
2023-02-06 19:54:04,690 - mmcls - INFO - Epoch [22][200/285]	lr: 1.000e-04, eta: 6:18:23, time: 1.423, data_time: 0.002, loss: 0.9021
2023-02-06 19:56:05,479 - mmcls - INFO - Saving checkpoint at 22 epochs
2023-02-06 19:56:43,107 - mmcls - INFO - Epoch(val) [22][72]	accuracy_top-1: 95.8042, accuracy_top-5: 100.0000
2023-02-06 19:59:07,509 - mmcls - INFO - Epoch [23][100/285]	lr: 1.000e-04, eta: 6:12:43, time: 1.444, data_time: 0.022, loss: 0.9288
2023-02-06 20:01:29,930 - mmcls - INFO - Epoch [23][200/285]	lr: 1.000e-04, eta: 6:13:23, time: 1.424, data_time: 0.002, loss: 0.9315
2023-02-06 20:03:30,051 - mmcls - INFO - Saving checkpoint at 23 epochs
2023-02-06 20:04:07,638 - mmcls - INFO - Epoch(val) [23][72]	accuracy_top-1: 95.6294, accuracy_top-5: 100.0000
2023-02-06 20:06:31,271 - mmcls - INFO - Epoch [24][100/285]	lr: 1.000e-04, eta: 6:07:48, time: 1.436, data_time: 0.022, loss: 0.9102
2023-02-06 20:08:53,303 - mmcls - INFO - Epoch [24][200/285]	lr: 1.000e-04, eta: 6:08:19, time: 1.420, data_time: 0.002, loss: 0.9120
2023-02-06 20:10:53,478 - mmcls - INFO - Saving checkpoint at 24 epochs
2023-02-06 20:11:31,016 - mmcls - INFO - Epoch(val) [24][72]	accuracy_top-1: 96.3287, accuracy_top-5: 100.0000
2023-02-06 20:13:54,263 - mmcls - INFO - Epoch [25][100/285]	lr: 1.000e-04, eta: 6:02:52, time: 1.432, data_time: 0.022, loss: 0.9224
2023-02-06 20:16:16,355 - mmcls - INFO - Epoch [25][200/285]	lr: 1.000e-04, eta: 6:03:17, time: 1.421, data_time: 0.002, loss: 0.8848
2023-02-06 20:18:16,845 - mmcls - INFO - Saving checkpoint at 25 epochs
2023-02-06 20:18:54,255 - mmcls - INFO - Epoch(val) [25][72]	accuracy_top-1: 97.2028, accuracy_top-5: 100.0000
2023-02-06 20:21:17,601 - mmcls - INFO - Epoch [26][100/285]	lr: 1.000e-04, eta: 5:57:57, time: 1.433, data_time: 0.022, loss: 0.8949
2023-02-06 20:23:40,030 - mmcls - INFO - Epoch [26][200/285]	lr: 1.000e-04, eta: 5:58:16, time: 1.424, data_time: 0.002, loss: 0.9291
2023-02-06 20:25:40,578 - mmcls - INFO - Saving checkpoint at 26 epochs
2023-02-06 20:26:18,098 - mmcls - INFO - Epoch(val) [26][72]	accuracy_top-1: 96.6783, accuracy_top-5: 100.0000
2023-02-06 20:28:41,918 - mmcls - INFO - Epoch [27][100/285]	lr: 1.000e-04, eta: 5:53:06, time: 1.438, data_time: 0.022, loss: 0.8760
2023-02-06 20:31:04,304 - mmcls - INFO - Epoch [27][200/285]	lr: 1.000e-04, eta: 5:53:18, time: 1.424, data_time: 0.002, loss: 0.8853
2023-02-06 20:33:04,888 - mmcls - INFO - Saving checkpoint at 27 epochs
2023-02-06 20:33:42,224 - mmcls - INFO - Epoch(val) [27][72]	accuracy_top-1: 96.5035, accuracy_top-5: 100.0000
2023-02-06 20:36:06,233 - mmcls - INFO - Epoch [28][100/285]	lr: 1.000e-04, eta: 5:48:15, time: 1.440, data_time: 0.022, loss: 0.9247
2023-02-06 20:38:28,598 - mmcls - INFO - Epoch [28][200/285]	lr: 1.000e-04, eta: 5:48:22, time: 1.424, data_time: 0.002, loss: 0.9075
2023-02-06 20:40:28,924 - mmcls - INFO - Saving checkpoint at 28 epochs
2023-02-06 20:41:06,548 - mmcls - INFO - Epoch(val) [28][72]	accuracy_top-1: 97.0280, accuracy_top-5: 100.0000
2023-02-06 20:43:29,789 - mmcls - INFO - Epoch [29][100/285]	lr: 1.000e-04, eta: 5:43:22, time: 1.432, data_time: 0.022, loss: 0.8922
2023-02-06 20:45:52,056 - mmcls - INFO - Epoch [29][200/285]	lr: 1.000e-04, eta: 5:43:24, time: 1.423, data_time: 0.002, loss: 0.9174
2023-02-06 20:47:52,253 - mmcls - INFO - Saving checkpoint at 29 epochs
2023-02-06 20:48:29,872 - mmcls - INFO - Epoch(val) [29][72]	accuracy_top-1: 97.2028, accuracy_top-5: 100.0000
2023-02-06 20:50:53,412 - mmcls - INFO - Epoch [30][100/285]	lr: 1.000e-04, eta: 5:38:30, time: 1.435, data_time: 0.022, loss: 0.9245
2023-02-06 20:53:15,869 - mmcls - INFO - Epoch [30][200/285]	lr: 1.000e-04, eta: 5:38:28, time: 1.425, data_time: 0.002, loss: 0.8963
2023-02-06 20:55:16,130 - mmcls - INFO - Saving checkpoint at 30 epochs
2023-02-06 20:55:53,703 - mmcls - INFO - Epoch(val) [30][72]	accuracy_top-1: 96.8531, accuracy_top-5: 100.0000
2023-02-06 20:58:17,524 - mmcls - INFO - Epoch [31][100/285]	lr: 1.000e-04, eta: 5:33:40, time: 1.438, data_time: 0.022, loss: 0.9195
2023-02-06 21:00:40,012 - mmcls - INFO - Epoch [31][200/285]	lr: 1.000e-04, eta: 5:33:33, time: 1.425, data_time: 0.002, loss: 0.8882
2023-02-06 21:02:40,363 - mmcls - INFO - Saving checkpoint at 31 epochs
2023-02-06 21:03:17,863 - mmcls - INFO - Epoch(val) [31][72]	accuracy_top-1: 96.6783, accuracy_top-5: 100.0000
2023-02-06 21:05:40,902 - mmcls - INFO - Epoch [32][100/285]	lr: 1.000e-04, eta: 5:28:48, time: 1.430, data_time: 0.022, loss: 0.8901
2023-02-06 21:08:03,012 - mmcls - INFO - Epoch [32][200/285]	lr: 1.000e-04, eta: 5:28:36, time: 1.421, data_time: 0.002, loss: 0.8842
2023-02-06 21:10:03,163 - mmcls - INFO - Saving checkpoint at 32 epochs
2023-02-06 21:10:40,847 - mmcls - INFO - Epoch(val) [32][72]	accuracy_top-1: 96.6783, accuracy_top-5: 100.0000
2023-02-06 21:13:04,203 - mmcls - INFO - Epoch [33][100/285]	lr: 1.000e-04, eta: 5:23:57, time: 1.433, data_time: 0.022, loss: 0.8973
2023-02-06 21:15:26,141 - mmcls - INFO - Epoch [33][200/285]	lr: 1.000e-04, eta: 5:23:41, time: 1.419, data_time: 0.002, loss: 0.8725
2023-02-06 21:17:26,459 - mmcls - INFO - Saving checkpoint at 33 epochs
2023-02-06 21:18:04,079 - mmcls - INFO - Epoch(val) [33][72]	accuracy_top-1: 96.5035, accuracy_top-5: 100.0000
2023-02-06 21:20:27,543 - mmcls - INFO - Epoch [34][100/285]	lr: 1.000e-04, eta: 5:19:06, time: 1.435, data_time: 0.022, loss: 0.8955
2023-02-06 21:22:49,015 - mmcls - INFO - Epoch [34][200/285]	lr: 1.000e-04, eta: 5:18:45, time: 1.415, data_time: 0.002, loss: 0.9060
2023-02-06 21:24:49,314 - mmcls - INFO - Saving checkpoint at 34 epochs
2023-02-06 21:25:26,672 - mmcls - INFO - Epoch(val) [34][72]	accuracy_top-1: 97.3776, accuracy_top-5: 100.0000
2023-02-06 21:27:49,949 - mmcls - INFO - Epoch [35][100/285]	lr: 1.000e-04, eta: 5:14:13, time: 1.433, data_time: 0.022, loss: 0.8999
